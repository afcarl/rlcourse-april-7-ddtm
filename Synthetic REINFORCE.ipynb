{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic gradients for REINFORCE\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this homework, I'm making an attempt to the reduce variance of the **REINFORCE** algorithm. The vanilla update rule has the following form:\n",
    "\n",
    "$$ \n",
    "    \\theta_{t + 1} = \\theta_{t} + \\alpha \\left( G_t - b(S_t) \\right) \\nabla_{\\theta} \\log \\pi(A_t \\, | \\, S_t, \\theta) \\, ,\n",
    "$$\n",
    "\n",
    "where $ b(S_t) $ is a baseline function. The update above corresponds to a one-sample gradient estimate of the performance metric $ \\eta(\\theta) = v_{\\pi_\\theta}(s_0) $:\n",
    "\n",
    "$$ \n",
    "    \\nabla \\eta(\\theta) = \\mathbb{E}_\\pi \\left[ G_t \\nabla_{\\theta} \\log \\pi(A_t \\, | \\, S_t, \\theta) \\right] \\, .\n",
    "$$\n",
    "\n",
    "One possible way of reducing variance is to introduce an additional network estimating\n",
    "\n",
    "$$ \n",
    "    \\mathbb{E}_\\pi \\left[ G_t \\nabla_{\\theta} \\log \\pi(A_t \\, | \\, S_t, \\theta) \\, | \\, S_t \\right] \\, .\n",
    "$$\n",
    "\n",
    "If we somehow managed to train it then we could use its outputs as a source of the policy gradient at each state encountered by the agent. This synthetic gradient is presumably less noisy as it incorporates information about past experiences.\n",
    "\n",
    "I'm guessing, for a similar purpose we could also construct an estimate for the conditional expectation $ \\mathbb{E}_\\pi \\left[ \\cdot \\, | \\, S_t, A_t \\right] $ but I'm not doing it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import signal\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from functools import partial\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from lib import plotting\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define main classes and funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_synthetic_grads(policy, decay_factor=0.995, learning_rate=0.2, scope=\"apply_synthetic_grads\"):\n",
    "    with tf.variable_scope(scope):\n",
    "        state_one_hot = tf.one_hot(policy.state, int(env.observation_space.n))\n",
    "        pred = tf.contrib.layers.fully_connected(\n",
    "            inputs=tf.expand_dims(state_one_hot, 0),\n",
    "            num_outputs=env.action_space.n,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=tf.zeros_initializer())\n",
    "        \n",
    "        # Compute REINFORCE gradient.\n",
    "        target = tf.gradients(policy.loss, policy.output_layer)[0]\n",
    "        target = tf.stop_gradient(target)\n",
    "\n",
    "        # We push synthetic predictions to the REINFORCE estimate.\n",
    "        loss = 0.5 * tf.reduce_sum(tf.square(pred - target))\n",
    "\n",
    "        # Compute surrogate policy loss that has known gradients.\n",
    "        pred = tf.stop_gradient(pred)\n",
    "        alpha = tf.Variable(1.0, trainable=False)\n",
    "        alpha = tf.assign(alpha, decay_factor * alpha)\n",
    "        policy_loss = tf.reduce_sum(tf.multiply(policy.output_layer, alpha * target + (1.0 - alpha) * pred))\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "        train_op = slim.learning.create_train_op(\n",
    "            loss, optimizer, global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        return policy_loss, train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Policy():    \n",
    "    def __init__(self, learning_rate=0.01, loss_modifier=None, scope=\"policy\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.action = tf.placeholder(dtype=tf.int32, name=\"action\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=env.action_space.n,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer())\n",
    "\n",
    "            self.action_probs = tf.squeeze(tf.nn.softmax(self.output_layer))\n",
    "            self.picked_action_prob = tf.gather(self.action_probs, self.action)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -tf.log(self.picked_action_prob) * self.target\n",
    "            \n",
    "            # Hack policy learning.\n",
    "            if loss_modifier is not None:\n",
    "                self.loss, loss_modifier_train_op = loss_modifier(self)\n",
    "            else:\n",
    "                loss_modifier_train_op = None\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = slim.learning.create_train_op(\n",
    "                self.loss, self.optimizer, global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "            if loss_modifier_train_op is not None:\n",
    "                self.train_op = self.train_op + loss_modifier_train_op\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.action_probs, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, action, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target, self.action: action  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ValueFunction():\n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_function\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.int32, [], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just table lookup estimator\n",
    "            state_one_hot = tf.one_hot(self.state, int(env.observation_space.n))\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(state_one_hot, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer())\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def reinforce(env, policy, value_function, num_episodes, epsilon=0.1, discount_factor=1.0):\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    try:\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Reset the environment and pick the fisrst action\n",
    "            state = env.reset()\n",
    "\n",
    "            episode = []\n",
    "\n",
    "            # One step in the environment\n",
    "            for t in itertools.count():\n",
    "\n",
    "                # Take a step\n",
    "                action_probs = policy.predict(state)\n",
    "                num_actions = len(action_probs)\n",
    "                action = np.random.choice(np.arange(num_actions), p=action_probs)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                # Keep track of the transition\n",
    "                episode.append(Transition(\n",
    "                    state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] = t + 1\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Go through the episode and make policy updates\n",
    "            for t, transition in enumerate(episode):\n",
    "                # The return after this timestep\n",
    "                total_return = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "                # Update our value estimator\n",
    "                value_function.update(transition.state, total_return)\n",
    "                # Calculate baseline/advantage\n",
    "                baseline_value = value_function.predict(transition.state)            \n",
    "                advantage = total_return - baseline_value\n",
    "                # Update our policy estimator\n",
    "                policy.update(transition.state, advantage, transition.action)\n",
    "    except Exception, e:\n",
    "        print e\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cliff walking experiment\n",
    "\n",
    "I'm basing my experiments on the [policy gradient implementation by Denny Britz](https://github.com/dennybritz/reinforcement-learning/tree/master/PolicyGradient). I feel like the cliff walking environment in conjunction with the tabular case is not the best setting for testing the idea but I figured I'd still give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_trials = 20\n",
    "num_episodes = 4000\n",
    "timeout = 2 * 60\n",
    "\n",
    "configs = {\n",
    "    'decay_factor': [1.0, 0.999],\n",
    "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "}\n",
    "\n",
    "def job(seed, decay_factor, learning_rate):\n",
    "    np.random.seed(seed=seed)\n",
    "    \n",
    "    class TimeoutException(Exception):\n",
    "        pass\n",
    "\n",
    "    def timeout_handler(signum, frame):\n",
    "        raise TimeoutException\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    loss_modifier = partial(apply_synthetic_grads, decay_factor=decay_factor, learning_rate=learning_rate)\n",
    "    policy = Policy(loss_modifier=loss_modifier)\n",
    "    value_function = ValueFunction()\n",
    "    \n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        signal.alarm(timeout)\n",
    "        try:\n",
    "            stats = reinforce(env, policy, value_function, num_episodes, discount_factor=1.0)\n",
    "        except Exception, e:\n",
    "            stats = plotting.EpisodeStats(\n",
    "                episode_lengths=np.zeros(num_episodes),\n",
    "                episode_rewards=np.full(num_episodes, np.nan))\n",
    "        else:\n",
    "            signal.alarm(0)\n",
    "        stats.episode_rewards[stats.episode_lengths == 0.0] = np.nan\n",
    "    \n",
    "    return stats\n",
    "\n",
    "for v in itertools.product(*configs.values()):\n",
    "    config = dict(zip(configs.keys(), v))\n",
    "    if config['decay_factor'] == 1.0:\n",
    "        config['learning_rate'] = 0.01\n",
    "    name = '{decay_factor}_{learning_rate}'.format(**config)\n",
    "    filename = 'results/{}.npy'.format(name)\n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "    all_stats = Parallel(n_jobs=8)(\n",
    "        [delayed(job)(s, **config) for s in np.random.randint(0, num_trials * 1000, num_trials)])\n",
    "    np.save(filename, np.array([np.nanmean(s.episode_rewards) for s in all_stats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_rewards = []\n",
    "keys = []\n",
    "names = configs.keys()\n",
    "for v in itertools.product(*configs.values()):\n",
    "    config = dict(zip(configs.keys(), v))\n",
    "    if config['decay_factor'] == 1.0:\n",
    "        config['learning_rate'] = 0.01\n",
    "    name = '{decay_factor}_{learning_rate}'.format(**config)\n",
    "    filename = 'results/{}.npy'.format(name)\n",
    "    avg_rewards.append(np.nanmean(np.load(filename)))\n",
    "    keys.append(tuple(config[k] for k in names))\n",
    "    \n",
    "s = pd.Series(avg_rewards, index=pd.MultiIndex.from_tuples(keys, names=names))\n",
    "s = s.reorder_levels(['decay_factor', 'learning_rate']).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Below I'm showing mean rewards for several settings of the decay factor and learning rate of the gradient synthesizer. The numbers are averaged across 20 run. I actually tried more different tuples of hyperparameters but none of them worked any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "decay_factor  learning_rate\n",
       "0.999         0.01            -104.103758\n",
       "              0.10             -85.140069\n",
       "              0.20             -79.067548\n",
       "              0.30             -79.587456\n",
       "1.000         0.01             -68.744480\n",
       "              0.01             -68.744480\n",
       "              0.01             -68.744480\n",
       "              0.01             -68.744480\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear that the proposed approach fails to show any improvement over vanilla REINFORCE with baseline. As I only spent **very** limited amount of time playing around with the idea, this outcome may not be representative. I see several directions for the future work:\n",
    "\n",
    "* Non-tabular case and different environments\n",
    "* Careful design of the synthesizer network\n",
    "* Optimization strategies for the synthesizer network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
